{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Motivations\n",
    "\n",
    "We want to model a high dimensional distribution $p(x)$. We can use Auto Regressive Models\n",
    "like Pixel CNN, Pixel RNN. However, the sampling is slow because every dimension\n",
    "is dependent of all previous dimensions, e.g. every pixel is dependent of all\n",
    "previous pixels.\n",
    "\n",
    "We can make some independence assumptions to make the sampling process faster.\n",
    "The extreme case is that every dimension is independent of all other dimensions.\n",
    "We want something in between. We assume that the dimensions are independent\n",
    "given some hidden variable $z$:\n",
    "\n",
    "$$p(x) = p_Z(z)p(x | z)$$\n",
    "\n",
    "This type of models is called *Latent Variable Models*.\n",
    "\n",
    "$p_Z(z)$ is the prior distribution over $z$ which is usually chosen to be a\n",
    "distribution that is easy to sample from, e.g. uniform or Gaussian.\n",
    "More complex $p_Z(z)$ allows more precise modeling of $p(x)$, e.g. $p_Z(z)$ is\n",
    " the output of a normalizing flow.\n",
    "Prior learning is still an open research area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "## Sampling\n",
    "To sample from this model, we first draw $z$ from $p_Z(z)$ and then sample $x$ from $p(x | z)$.\n",
    "$p(x | z)$ is usually a neural network with parameter $\\theta$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "z \\sim p_Z(z) \\\\\n",
    "x \\sim p_\\theta (x | z)\n",
    "\\end{eqnarray}\n",
    "\n",
    "## Evaluating likelihood\n",
    "Given a datapoint $x_0$ and a model $p_\\theta (x)$, we can evaluate the likelihood of $x_0$.\n",
    "Some applications of this: to detect out of distribution (OOD) datapoints.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p_\\theta (x_0) = \\sum_{z \\sim p_Z} p_Z(z) p_\\theta(x|z)\n",
    "\\end{eqnarray}\n",
    "\n",
    "## Training\n",
    "LVMs are trained by maximum likelihood\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\underset{\\theta}{\\max} \\sum_i \\log p_\\theta(x^{(i)}) = \\sum_i \\log \\sum_z p_Z(z) p_\\theta(x^{(i)} | z)\n",
    "\\end{eqnarray}\n",
    "\n",
    "## Representation\n",
    "We want $z$ to capture properties of $x$. A good representation is useful in many other problems, e.g. RL,\n",
    "supervised learning, disentanglement, ...\n",
    "\n",
    "$$x \\rightarrow z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training LVMs\n",
    "Given a training dataset $\\mathcal{D} = \\{ x^{(i)} \\}_{i=1}^n$, we need to find parameter $\\theta$ s.t.\n",
    "the likelihood of the training data is maximized. The first step is to get an estimate of\n",
    "the likelihood. We consider the prior sampling method:\n",
    "## Prior sampling\n",
    "Let's consider an example where the data is in 2D and $z$ is 1D.\n",
    "The data consist of $10$ clusters as shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff64b2ddc40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "mu = np.random.random((10, 2)) * 20\n",
    "data = []\n",
    "for i in range(10):\n",
    "    data.append(mu[i] + np.random.random((10, 2)) * 2)\n",
    "data = np.concatenate(data, axis=0)\n",
    "ax.scatter(data[:, 0], data[:, 1])\n",
    "ax.scatter(data[0, 0], data[0, 1], c='r')\n",
    "\n",
    "ax.contourf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the mixture of Gaussians model:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p_\\theta(x) = \\sum_{i = 1}^{10} p_Z(z) \\mathcal{N}(x | \\mu = f^{(i)}(z), \\Sigma = g^{(i)}(z))\n",
    "\\end{eqnarray}\n",
    "\n",
    "Assume that we have fitted the model to the data, i.e. each cluster is modeled by a\n",
    " Gaussian with mean $\\mu^{i}$ and covariance $\\Sigma^{(i)}$.\n",
    "\n",
    "Given the red datapoint $x^{(0)} \\sim \\mathcal{N}(\\mu^{(1)}, \\Sigma^{(1)})$, we want to estimate $p_\\theta(x^{(0)})$.\n",
    "If we sample $z$ uniformly, 9 out of 10 times,\n",
    "the selected a component which is not $\\mathcal{N}(\\mu^{(1)}, \\Sigma^{(1)})$ and it will assign very low\n",
    "probability to $x^{(0)}$. Because of that, in most cases (90%), the gradient signal is very small and unusable.\n",
    "The situation is worse in higher dimensional space. The probability that a randomly sampled $z$ end up producing\n",
    "the component near $x^{(0)}$ is practically $0$.\n",
    "\n",
    "We want to sample $z$ such that the probability of $x^{0}$ under the model is\n",
    "high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importance sampling\n",
    "\n",
    "We want to compute $\\mathbb{E}_{p(x)} p(x) f(x)$. However, $p(x)$ is\n",
    "- Hard to sample from, and/or\n",
    "- Sample from $p(x)$ is not informative. For example, most of the density is\n",
    "concentrated at the region where $f(x) \\approx 0$. This is more visible for\n",
    "high dimensional distributions.\n",
    "\n",
    "We use a *proposal distribution* $q(x)$ that is\n",
    "- Easy to sample from, and\n",
    "- Samples from $q(x)$ is more informative, i.e. most of the density is\n",
    "concentrated at the region where $f(x)$ is large.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{p(x)} p(x) f(x) & = & \\sum_{x} p(x) f(x) \\\\\n",
    "& = & \\sum_{x} q(x) \\frac{p(x)}{q(x)} f(x) \\\\\n",
    "& = & \\mathbb{E}_{q(x)} \\frac{p(x)}{q(x)} f(x) \\\\\n",
    "& \\approx & \\frac{1}{K} \\sum_{i = 1}^K \\frac{p(x^{(i)})}{q(x^{(i)})} f(x^{(i)}), \\text{ where } x^{(i)} \\sim q(x)\n",
    "\\end{eqnarray}\n",
    "\n",
    "For LVM objective, $f(x) = \\log p_\\theta(x|z)$, and samples are taken from $p_Z(z)$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{i} \\log \\sum_z p_Z(z) p_\\theta (x^{{i}} | z) \\approx \\sum_i \\log \\frac{1}{K} \\sum_k^K \\frac{p_Z(z^{(i)}_k)}{q_Z(z^{(i)}_{k})} p(x^{(i)}|z^{(i)}_{k}),\n",
    "\\end{eqnarray}\n",
    "where $z^{(i)}_k \\sim q(z)$.\n",
    "\n",
    "Question: what is the good $q(z)$ given $x^{(i)}$?\n",
    "\n",
    "Answer: $q^{(i)}(z) = p_\\theta(z|x^{(i)})$. This is justified through the KL divergence\n",
    "formulation.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "q^{(i)}(z) = p_\\theta (z | x^{(i)}) = \\frac{p_\\theta (x^{(i)}|z) p_Z(z)}{p_\\theta (x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can optimize $q^{(i)}(z)$ by minimizing the KL divergence between\n",
    "$q^{(i)}(z)$ and $p(z | x^{(i)})$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "& & \\underset{q(z)}{\\min} KL(q(z) || p_\\theta (z | x^{(i)}) \\\\\n",
    "& = & \\underset{q(z)}{\\min} \\mathbb{E}_{q(z)} \\left[ \\log \\frac{q(z)}{p_\\theta(z | x^{(i)})} \\right] \\\\\n",
    "& = & \\underset{q(z)}{\\min} \\mathbb{E}_{q(z)} \\left[ \\log \\frac{q(z) p_\\theta(x^{(i)})}{p_\\theta(x^{(i)} | z) p_Z(z)} \\right] \\\\\n",
    "& = & \\underset{q(z)}{\\min} \\mathbb{E}_{q(z)} \\left[ \\log q(z) - \\log p_\\theta(x^{(i)} | z) - \\log p_Z(z) \\right] + C\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can optimize the parameter of $q(z)$ to find the optimal variational distribution.\n",
    "One method for doing this is called Stochastic Variational Inference.\n",
    "However, running SVI for each datapoint $x^{(i)}$ is expensive.\n",
    "\n",
    "Instead of solving $n$ optimization problems, we learn a model that solves\n",
    "the variational inference problem given the datapoint $x$, i.e. a function\n",
    "from $x$ to $z$. This is called Amortized Variational Inference. VAEs follow\n",
    "this approach. The function is implemented by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Amortized Variational Inference\n",
    "We parametrize $q$ with $\\phi$, the problem becomes\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&  & \\underset{\\phi}{\\min} \\sum_i KL(q_\\phi(z) || p_\\theta (z | x^{(i)}) \\\\\n",
    "& = & \\underset{\\phi}{\\min} \\sum_i \\mathbb{E} \\left[ \\log \\frac{q_\\phi(z)}{p_\\theta(z | x^{(i)}} \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "The problem becomes finding $q_\\phi$ that minimizes an expectation. Approaches to\n",
    "this problem are:\n",
    "- Likelihood Ratio Gradient: tends to have high variance for high dimensional distributions (see examples)\n",
    "- Path Wise Derivative (Reparameterization Trick): is easily applicable to only a limited number of\n",
    "distributions, e.g. Gaussian, Poisson, ... Can be applied to more complicated\n",
    "distributions via the use of flow models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importance Weighted AutoEncoder (IWAE)\n",
    "\n",
    "The objectives for IWAE are\n",
    "\n",
    "- The maximum likelihood objective\n",
    "\n",
    "$$\\frac{1}{K} \\sum_i \\frac{p_Z(z^{(i)}_k)}{q_\\phi (z^{(i)}_k)} p_\\theta(x^{(i)} | z^{(i)}_k) \\text{ where } z^{(i)}_k \\sim q_\\phi(z; x^{(i)})$$\n",
    "\n",
    "- The KL divergence\n",
    "\n",
    "$$\\underset{\\phi}{\\min} \\sum_i KL\\left(q_\\phi\\left(z; x^{(i)}\\right) || p_\\theta\\left(z | x^{(i)}\\right)\\right)$$\n",
    "\n",
    "We can jointly optimize the 2 objectives by maximizing the difference between the\n",
    "likelihood and the KL."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}